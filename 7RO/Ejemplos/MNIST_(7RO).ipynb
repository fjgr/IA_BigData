{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8n4QYykjr89VmOxKqu73H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjgr/IA_BigData/blob/main/7RO/Ejemplos/MNIST_(7RO).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Generar el dataset (peso, altura, IMC)\n",
        "np.random.seed(42)\n",
        "def generar_dataset(num_filas):\n",
        "    alturas = np.round(np.random.uniform(1.4, 2.0, num_filas), 2)\n",
        "    pesos = np.round(np.random.uniform(40, 120, num_filas), 1)\n",
        "    imc = np.round(pesos / (alturas ** 2), 2)\n",
        "    return pd.DataFrame({'peso': pesos, 'altura': alturas, 'IMC': imc})\n",
        "\n",
        "# Crear dataset de 1000 filas\n",
        "dataset = generar_dataset(1000)\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X = dataset[['peso', 'altura']].values  # Entradas: peso y altura\n",
        "y = dataset['IMC'].values  # Salida: IMC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar las entradas\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Definir la arquitectura de la red neuronal\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(2,)),  # Capa oculta con 64 neuronas\n",
        "    Dense(32, activation='relu'),                   # Segunda capa oculta con 32 neuronas\n",
        "    Dense(1, activation='linear')                   # Capa de salida para regresión\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Pérdida (MSE): {loss:.4f}, Error Absoluto Medio (MAE): {mae:.4f}\")\n",
        "\n",
        "# Realizar predicciones\n",
        "nuevos_datos = np.array([[70, 1.75], [90, 1.8]])  # Ejemplo de peso y altura\n",
        "nuevos_datos = scaler.transform(nuevos_datos)\n",
        "predicciones = model.predict(nuevos_datos)\n",
        "\n",
        "print(\"Predicción de IMC para nuevos datos:\")\n",
        "for i, imc_pred in enumerate(predicciones):\n",
        "    print(f\"Peso: {nuevos_datos[i][0]:.2f}, Altura: {nuevos_datos[i][1]:.2f}, IMC Predicho: {imc_pred[0]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F50n__wDOrB",
        "outputId": "6cdc8272-d5c7-48af-8fae-77970fe3de69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 896.7706 - mae: 28.1378 - val_loss: 731.9659 - val_mae: 25.0116\n",
            "Epoch 2/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 660.5767 - mae: 23.4033 - val_loss: 331.8384 - val_mae: 15.5452\n",
            "Epoch 3/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 265.6688 - mae: 13.7000 - val_loss: 51.4743 - val_mae: 6.4024\n",
            "Epoch 4/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 34.4831 - mae: 4.9793 - val_loss: 23.5029 - val_mae: 3.9286\n",
            "Epoch 5/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 20.3276 - mae: 3.6836 - val_loss: 19.3361 - val_mae: 3.5483\n",
            "Epoch 6/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16.4563 - mae: 3.3275 - val_loss: 16.1158 - val_mae: 3.2537\n",
            "Epoch 7/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15.2370 - mae: 3.1905 - val_loss: 13.2353 - val_mae: 2.9423\n",
            "Epoch 8/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.5058 - mae: 2.7582 - val_loss: 10.8195 - val_mae: 2.6744\n",
            "Epoch 9/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5257 - mae: 2.5068 - val_loss: 8.7443 - val_mae: 2.3657\n",
            "Epoch 10/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9648 - mae: 2.2732 - val_loss: 6.8487 - val_mae: 2.0831\n",
            "Epoch 11/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1265 - mae: 1.9938 - val_loss: 5.4091 - val_mae: 1.8285\n",
            "Epoch 12/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6231 - mae: 1.6865 - val_loss: 4.2407 - val_mae: 1.5990\n",
            "Epoch 13/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6083 - mae: 1.4755 - val_loss: 3.2458 - val_mae: 1.3880\n",
            "Epoch 14/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1612 - mae: 1.3840 - val_loss: 2.6479 - val_mae: 1.2324\n",
            "Epoch 15/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3567 - mae: 1.1420 - val_loss: 2.0182 - val_mae: 1.0647\n",
            "Epoch 16/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8481 - mae: 1.0247 - val_loss: 1.6245 - val_mae: 0.9516\n",
            "Epoch 17/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3390 - mae: 0.8653 - val_loss: 1.3018 - val_mae: 0.8668\n",
            "Epoch 18/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2214 - mae: 0.8508 - val_loss: 1.0903 - val_mae: 0.8088\n",
            "Epoch 19/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0184 - mae: 0.7728 - val_loss: 0.9042 - val_mae: 0.7466\n",
            "Epoch 20/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8314 - mae: 0.7372 - val_loss: 0.7621 - val_mae: 0.6865\n",
            "Epoch 21/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7168 - mae: 0.6710 - val_loss: 0.6641 - val_mae: 0.6446\n",
            "Epoch 22/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5839 - mae: 0.6114 - val_loss: 0.6017 - val_mae: 0.6392\n",
            "Epoch 23/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5742 - mae: 0.6174 - val_loss: 0.5276 - val_mae: 0.5953\n",
            "Epoch 24/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5038 - mae: 0.5796 - val_loss: 0.4581 - val_mae: 0.5504\n",
            "Epoch 25/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4172 - mae: 0.5220 - val_loss: 0.4057 - val_mae: 0.5222\n",
            "Epoch 26/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3718 - mae: 0.4914 - val_loss: 0.3644 - val_mae: 0.4973\n",
            "Epoch 27/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3612 - mae: 0.4961 - val_loss: 0.3206 - val_mae: 0.4484\n",
            "Epoch 28/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3566 - mae: 0.4821 - val_loss: 0.2832 - val_mae: 0.4320\n",
            "Epoch 29/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3012 - mae: 0.4452 - val_loss: 0.2749 - val_mae: 0.4147\n",
            "Epoch 30/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2749 - mae: 0.4216 - val_loss: 0.2319 - val_mae: 0.3911\n",
            "Epoch 31/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2293 - mae: 0.3908 - val_loss: 0.2141 - val_mae: 0.3782\n",
            "Epoch 32/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2361 - mae: 0.3864 - val_loss: 0.2028 - val_mae: 0.3625\n",
            "Epoch 33/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1969 - mae: 0.3549 - val_loss: 0.1805 - val_mae: 0.3473\n",
            "Epoch 34/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1952 - mae: 0.3566 - val_loss: 0.1750 - val_mae: 0.3408\n",
            "Epoch 35/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1810 - mae: 0.3481 - val_loss: 0.1655 - val_mae: 0.3315\n",
            "Epoch 36/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1625 - mae: 0.3279 - val_loss: 0.1599 - val_mae: 0.3259\n",
            "Epoch 37/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1610 - mae: 0.3299 - val_loss: 0.1432 - val_mae: 0.3095\n",
            "Epoch 38/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1461 - mae: 0.3023 - val_loss: 0.1348 - val_mae: 0.3016\n",
            "Epoch 39/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1428 - mae: 0.3007 - val_loss: 0.1242 - val_mae: 0.2917\n",
            "Epoch 40/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1347 - mae: 0.2965 - val_loss: 0.1187 - val_mae: 0.2829\n",
            "Epoch 41/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1230 - mae: 0.2790 - val_loss: 0.1133 - val_mae: 0.2791\n",
            "Epoch 42/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1120 - mae: 0.2735 - val_loss: 0.1133 - val_mae: 0.2712\n",
            "Epoch 43/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1151 - mae: 0.2794 - val_loss: 0.1078 - val_mae: 0.2697\n",
            "Epoch 44/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1026 - mae: 0.2593 - val_loss: 0.1037 - val_mae: 0.2597\n",
            "Epoch 45/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1038 - mae: 0.2576 - val_loss: 0.1003 - val_mae: 0.2579\n",
            "Epoch 46/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1043 - mae: 0.2597 - val_loss: 0.0915 - val_mae: 0.2503\n",
            "Epoch 47/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1022 - mae: 0.2600 - val_loss: 0.0835 - val_mae: 0.2395\n",
            "Epoch 48/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0820 - mae: 0.2362 - val_loss: 0.0894 - val_mae: 0.2482\n",
            "Epoch 49/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0892 - mae: 0.2451 - val_loss: 0.0829 - val_mae: 0.2372\n",
            "Epoch 50/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0803 - mae: 0.2320 - val_loss: 0.0733 - val_mae: 0.2219\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0667 - mae: 0.2140 \n",
            "Pérdida (MSE): 0.0733, Error Absoluto Medio (MAE): 0.2219\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Predicción de IMC para nuevos datos:\n",
            "Peso: -0.48, Altura: 0.30, IMC Predicho: 23.03\n",
            "Peso: 0.38, Altura: 0.58, IMC Predicho: 27.61\n"
          ]
        }
      ]
    }
  ]
}