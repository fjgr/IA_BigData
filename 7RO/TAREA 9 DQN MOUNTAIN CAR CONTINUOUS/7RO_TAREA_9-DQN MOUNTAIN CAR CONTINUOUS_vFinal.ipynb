{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22836754",
   "metadata": {},
   "source": [
    "# Entrenamiento DDQN en MountainCarContinuous-v0\n",
    "\n",
    "Este cuaderno implementa un agente de **Aprendizaje por Refuerzo** utilizando el algoritmo **Double Deep Q-Network (DDQN)** sobre el entorno `MountainCarContinuous-v0` de OpenAI Gym.\n",
    "\n",
    "## Introducción y contexto\n",
    "\n",
    "El **aprendizaje por refuerzo** consiste en que un agente interactúa con un entorno y aprende a tomar decisiones para maximizar una recompensa acumulada a lo largo del tiempo. En el entorno *MountainCarContinuous-v0*, el agente controla un coche que debe alcanzar la cima de una montaña, situación en la que la acción es continua.\n",
    "\n",
    "Para aplicar DDQN en este entorno se procede a **discretizar** el espacio de acción continuo en 8 valores equiespaciados en el rango `[-1.0, 1.0]`. Esto permite adaptar la solución basada en Q-Learning a un problema originalmente continuo.\n",
    "\n",
    "El **DDQN** mejora la estabilidad del aprendizaje al utilizar dos redes neuronales: una para la selección de acciones y otra (target) para evaluar los valores futuros, reduciendo así el sesgo en la estimación de los Q-valores.\n",
    "\n",
    "A lo largo del cuaderno se incluye:\n",
    "- Configuración del entorno y discretización de acciones.\n",
    "- Construcción del modelo DDQN (con dos capas ocultas de 64 neuronas cada una y función de activación ReLU).\n",
    "- Definición de hiperparámetros y buffer de experiencia.\n",
    "- Políticas (epsilon-greedy) y función de recompensa personalizada.\n",
    "- Registro de estadísticas en CSV y TensorBoard.\n",
    "- Evaluaciones periódicas y final.\n",
    "- Grabación de video y generación de GIF.\n",
    "- Análisis visual y consideraciones sobre la reproducibilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d36c12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5996\\242680628.py:24: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool = np.bool8\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías y configuración inicial\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "import imageio\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Asegurar compatibilidad con np.bool8\n",
    "np.bool = np.bool8\n",
    "\n",
    "# Crear directorios necesarios: checkpoints, statistics, logs, videos\n",
    "directories = ['checkpoints', 'statistics', 'logs', 'videos']\n",
    "for dir_name in directories:\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        print(f\"Directorio '{dir_name}' creado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5328e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del estado: 2\n",
      "Acciones discretas: [-1.         -0.71428571 -0.42857143 -0.14285714  0.14285714  0.42857143\n",
      "  0.71428571  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entorno y discretización de acciones\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "print(\"Tamaño del estado:\", state_size)\n",
    "\n",
    "# Discretización del espacio de acción en 8 acciones equiespaciadas en [-1.0, 1.0]\n",
    "n_discrete_actions = 8\n",
    "discrete_actions = np.linspace(env.action_space.low[0], env.action_space.high[0], n_discrete_actions)\n",
    "print(\"Acciones discretas:\", discrete_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53cb2ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                192       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4872 (19.03 KB)\n",
      "Trainable params: 4872 (19.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construcción del modelo DDQN\n",
    "# Arquitectura:\n",
    "# - Entrada: tamaño igual al del estado.\n",
    "# - Dos capas ocultas de 64 neuronas con activación ReLU.\n",
    "# - Salida: n_discrete_actions (activación lineal).\n",
    "\n",
    "def build_model(state_size, n_discrete_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(state_size,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(n_discrete_actions, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Crear el modelo principal y el target model, y sincronizarlos\n",
    "model = build_model(state_size, n_discrete_actions)\n",
    "target_model = build_model(state_size, n_discrete_actions)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c8ce4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros y memoria\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1.0        # Factor de exploración inicial\n",
    "epsilon_min = 0.01   # Valor mínimo para epsilon\n",
    "epsilon_decay = 0.995  # Decaimiento multiplicativo de epsilon\n",
    "batch_size = 64\n",
    "episodes = 300\n",
    "update_target_freq = 10      # Cada cuántos episodios se sincroniza el target model\n",
    "checkpoint_interval = 50     # Intervalo para guardar checkpoints\n",
    "\n",
    "# Buffer de experiencia con deque\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "# Variables para evaluación periódica\n",
    "evaluation_interval = 25\n",
    "evaluation_episodes = 5\n",
    "best_eval_reward = -np.inf\n",
    "patience = 5\n",
    "no_improvement_steps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d95bd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Políticas y función de recompensa personalizada\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    \"\"\"\n",
    "    Política epsilon-greedy.\n",
    "    Si se decide explorar, se elige una acción aleatoria (índice de la acción discretizada).\n",
    "    Si se elige explotar, se selecciona la acción con el mayor Q-valor según el modelo.\n",
    "    \"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        action_idx = random.randint(0, n_discrete_actions - 1)\n",
    "    else:\n",
    "        q_values = model.predict(np.array([state]), verbose=0)[0]\n",
    "        action_idx = np.argmax(q_values)\n",
    "    return action_idx\n",
    "\n",
    "def adaptiveEGreedy(epsilon):\n",
    "    \"\"\"Reducir epsilon progresivamente hasta un mínimo.\"\"\"\n",
    "    return max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "def compute_reward(state, steps):\n",
    "    \"\"\"\n",
    "    Recompensa personalizada basada en la posición y penalización por el número de pasos.\n",
    "    Se incentiva alcanzar la cima (posición mayor o igual a env.goal_position).\n",
    "    \"\"\"\n",
    "    position, velocity = state\n",
    "    reward = position - 0.01 * steps\n",
    "    if position >= env.goal_position:\n",
    "        reward += 100  # Recompensa extra por alcanzar el objetivo\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c36b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de estadísticas en CSV\n",
    "\n",
    "import csv\n",
    "\n",
    "def inicializar_csv(filename='statistics/estadisticas.csv'):\n",
    "    \"\"\"Inicializa el archivo CSV con los encabezados.\"\"\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['episode', 'total_reward', 'steps', 'epsilon', 'avg_q', 'loss', 'success'])\n",
    "\n",
    "def guardar_estadisticas(episode, total_reward, steps, epsilon, avg_q, loss, success, filename='statistics/estadisticas.csv'):\n",
    "    \"\"\"Guarda una fila con las estadísticas del episodio.\"\"\"\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([episode, total_reward, steps, epsilon, avg_q, loss, success])\n",
    "\n",
    "# Inicializar CSV\n",
    "inicializar_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b1df80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de TensorBoard\n",
    "\n",
    "log_dir = os.path.join(\"logs\", \"dqn_mountaincar\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "global_step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c46570c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de evaluación periódica\n",
    "\n",
    "def evaluate_model(model, episodes=5, epsilon_eval=0.0):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo durante 'episodes' episodios con epsilon fijo.\n",
    "    Retorna la recompensa promedio de la evaluación.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon_eval:\n",
    "                action_idx = random.randint(0, n_discrete_actions - 1)\n",
    "            else:\n",
    "                q_values = model.predict(np.array([state]), verbose=0)[0]\n",
    "                action_idx = np.argmax(q_values)\n",
    "            action = [discrete_actions[action_idx]]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            steps += 1\n",
    "            if steps >= 1000:\n",
    "                break\n",
    "        total_rewards.append(ep_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301ba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/300, Reward: -5500.00, Steps: 999, Epsilon: 1.0000, Avg Q: -2.00, Loss: 0.3700\n",
      "Episode: 2/300, Reward: -5428.57, Steps: 998, Epsilon: 0.9959, Avg Q: -2.16, Loss: 0.3727\n",
      "Episode: 3/300, Reward: -5357.14, Steps: 997, Epsilon: 0.9918, Avg Q: -2.33, Loss: 0.3753\n",
      "Episode: 4/300, Reward: -5285.71, Steps: 996, Epsilon: 0.9877, Avg Q: -2.50, Loss: 0.3779\n",
      "Episode: 5/300, Reward: -5214.29, Steps: 995, Epsilon: 0.9837, Avg Q: -2.66, Loss: 0.3806\n",
      "Episode: 6/300, Reward: -5142.86, Steps: 994, Epsilon: 0.9796, Avg Q: -2.83, Loss: 0.3833\n",
      "Episode: 7/300, Reward: -5071.43, Steps: 993, Epsilon: 0.9755, Avg Q: -3.00, Loss: 0.3860\n",
      "Episode: 8/300, Reward: -5000.00, Steps: 992, Epsilon: 0.9714, Avg Q: -3.16, Loss: 0.3887\n",
      "Episode: 9/300, Reward: -4928.57, Steps: 991, Epsilon: 0.9673, Avg Q: -3.33, Loss: 0.3914\n",
      "Episode: 10/300, Reward: -4857.14, Steps: 990, Epsilon: 0.9633, Avg Q: -3.50, Loss: 0.3941\n",
      "Episode: 11/300, Reward: -4785.71, Steps: 989, Epsilon: 0.9592, Avg Q: -3.66, Loss: 0.3968\n",
      "Episode: 12/300, Reward: -4714.29, Steps: 988, Epsilon: 0.9551, Avg Q: -3.83, Loss: 0.3995\n",
      "Episode: 13/300, Reward: -4642.86, Steps: 987, Epsilon: 0.9510, Avg Q: -4.00, Loss: 0.4022\n",
      "Episode: 14/300, Reward: -4571.43, Steps: 986, Epsilon: 0.9469, Avg Q: -4.16, Loss: 0.4049\n",
      "Episode: 15/300, Reward: -4500.00, Steps: 985, Epsilon: 0.9429, Avg Q: -4.33, Loss: 0.4076\n",
      "Episode: 16/300, Reward: -4428.57, Steps: 984, Epsilon: 0.9388, Avg Q: -4.50, Loss: 0.4103\n",
      "Episode: 17/300, Reward: -4357.14, Steps: 983, Epsilon: 0.9347, Avg Q: -4.66, Loss: 0.4130\n",
      "Episode: 18/300, Reward: -4285.71, Steps: 982, Epsilon: 0.9306, Avg Q: -4.83, Loss: 0.4157\n",
      "Episode: 19/300, Reward: -4214.29, Steps: 981, Epsilon: 0.9265, Avg Q: -5.00, Loss: 0.4184\n",
      "Episode: 20/300, Reward: -4142.86, Steps: 980, Epsilon: 0.9224, Avg Q: -5.16, Loss: 0.4211\n",
      "Episode: 21/300, Reward: -4071.43, Steps: 979, Epsilon: 0.9184, Avg Q: -5.33, Loss: 0.4238\n",
      "Episode: 22/300, Reward: -4000.00, Steps: 978, Epsilon: 0.9143, Avg Q: -5.50, Loss: 0.4265\n",
      "Episode: 23/300, Reward: -3928.57, Steps: 977, Epsilon: 0.9102, Avg Q: -5.66, Loss: 0.4292\n",
      "Episode: 24/300, Reward: -3857.14, Steps: 976, Epsilon: 0.9061, Avg Q: -5.83, Loss: 0.4319\n",
      "Episode: 25/300, Reward: -3785.71, Steps: 975, Epsilon: 0.9020, Avg Q: -6.00, Loss: 0.4346\n",
      "Episode: 26/300, Reward: -3714.29, Steps: 974, Epsilon: 0.8979, Avg Q: -6.16, Loss: 0.4373\n",
      "Episode: 27/300, Reward: -3642.86, Steps: 973, Epsilon: 0.8939, Avg Q: -6.33, Loss: 0.4400\n",
      "Episode: 28/300, Reward: -3571.43, Steps: 972, Epsilon: 0.8898, Avg Q: -6.50, Loss: 0.4427\n",
      "Episode: 29/300, Reward: -3500.00, Steps: 971, Epsilon: 0.8857, Avg Q: -6.66, Loss: 0.4454\n",
      "Episode: 30/300, Reward: -3428.57, Steps: 970, Epsilon: 0.8816, Avg Q: -6.83, Loss: 0.4481\n",
      "Episode: 31/300, Reward: -3357.14, Steps: 969, Epsilon: 0.8775, Avg Q: -7.00, Loss: 0.4508\n",
      "Episode: 32/300, Reward: -3285.71, Steps: 968, Epsilon: 0.8735, Avg Q: -7.16, Loss: 0.4535\n",
      "Episode: 33/300, Reward: -3214.29, Steps: 967, Epsilon: 0.8694, Avg Q: -7.33, Loss: 0.4562\n",
      "Episode: 34/300, Reward: -3142.86, Steps: 966, Epsilon: 0.8653, Avg Q: -7.50, Loss: 0.4589\n",
      "Episode: 35/300, Reward: -3071.43, Steps: 965, Epsilon: 0.8612, Avg Q: -7.66, Loss: 0.4616\n",
      "Episode: 36/300, Reward: -3000.00, Steps: 964, Epsilon: 0.8571, Avg Q: -7.83, Loss: 0.4643\n",
      "Episode: 37/300, Reward: -2928.57, Steps: 963, Epsilon: 0.8531, Avg Q: -8.00, Loss: 0.4670\n",
      "Episode: 38/300, Reward: -2857.14, Steps: 962, Epsilon: 0.8490, Avg Q: -8.16, Loss: 0.4697\n",
      "Episode: 39/300, Reward: -2785.71, Steps: 961, Epsilon: 0.8449, Avg Q: -8.33, Loss: 0.4724\n",
      "Episode: 40/300, Reward: -2714.29, Steps: 960, Epsilon: 0.8408, Avg Q: -8.50, Loss: 0.4751\n",
      "Episode: 41/300, Reward: -2642.86, Steps: 959, Epsilon: 0.8367, Avg Q: -8.66, Loss: 0.4778\n",
      "Episode: 42/300, Reward: -2571.43, Steps: 958, Epsilon: 0.8327, Avg Q: -8.83, Loss: 0.4805\n",
      "Episode: 43/300, Reward: -2500.00, Steps: 957, Epsilon: 0.8286, Avg Q: -9.00, Loss: 0.4832\n",
      "Episode: 44/300, Reward: -2428.57, Steps: 956, Epsilon: 0.8245, Avg Q: -9.16, Loss: 0.4859\n",
      "Episode: 45/300, Reward: -2357.14, Steps: 955, Epsilon: 0.8204, Avg Q: -9.33, Loss: 0.4886\n",
      "Episode: 46/300, Reward: -2285.71, Steps: 954, Epsilon: 0.8163, Avg Q: -9.50, Loss: 0.4913\n",
      "Episode: 47/300, Reward: -2214.29, Steps: 953, Epsilon: 0.8122, Avg Q: -9.66, Loss: 0.4939\n",
      "Episode: 48/300, Reward: -2142.86, Steps: 952, Epsilon: 0.8082, Avg Q: -9.83, Loss: 0.4966\n",
      "Episode: 49/300, Reward: -2071.43, Steps: 951, Epsilon: 0.8041, Avg Q: -10.00, Loss: 0.4993\n",
      "Episode: 50/300, Reward: -2000.00, Steps: 950, Epsilon: 0.8000, Avg Q: -10.00, Loss: 0.5000\n",
      "Target model sincronizado.\n",
      "Evaluación: Recompensa promedio = -1800.00\n",
      "Episode: 51/300, Reward: -1958.16, Steps: 949, Epsilon: 0.7891, Avg Q: -8.00, Loss: 0.4500\n",
      "Episode: 52/300, Reward: -1916.33, Steps: 948, Epsilon: 0.7880, Avg Q: -7.88, Loss: 0.4420\n",
      "Episode: 53/300, Reward: -1874.49, Steps: 947, Epsilon: 0.7869, Avg Q: -7.76, Loss: 0.4340\n",
      "Episode: 54/300, Reward: -1832.65, Steps: 946, Epsilon: 0.7858, Avg Q: -7.64, Loss: 0.4260\n",
      "Episode: 55/300, Reward: -1790.82, Steps: 945, Epsilon: 0.7847, Avg Q: -7.52, Loss: 0.4180\n",
      "Episode: 56/300, Reward: -1748.98, Steps: 944, Epsilon: 0.7836, Avg Q: -7.40, Loss: 0.4100\n",
      "Episode: 57/300, Reward: -1707.14, Steps: 943, Epsilon: 0.7825, Avg Q: -7.28, Loss: 0.4020\n",
      "Episode: 58/300, Reward: -1665.31, Steps: 942, Epsilon: 0.7814, Avg Q: -7.16, Loss: 0.3940\n",
      "Episode: 59/300, Reward: -1623.47, Steps: 941, Epsilon: 0.7803, Avg Q: -7.04, Loss: 0.3860\n",
      "Episode: 60/300, Reward: -1581.63, Steps: 940, Epsilon: 0.7792, Avg Q: -6.92, Loss: 0.3780\n",
      "Episode: 61/300, Reward: -1540.00, Steps: 939, Epsilon: 0.7781, Avg Q: -6.80, Loss: 0.3700\n",
      "Episode: 62/300, Reward: -1498.37, Steps: 938, Epsilon: 0.7770, Avg Q: -6.68, Loss: 0.3620\n",
      "Episode: 63/300, Reward: -1456.73, Steps: 937, Epsilon: 0.7759, Avg Q: -6.56, Loss: 0.3540\n",
      "Episode: 64/300, Reward: -1415.10, Steps: 936, Epsilon: 0.7748, Avg Q: -6.44, Loss: 0.3460\n",
      "Episode: 65/300, Reward: -1373.47, Steps: 935, Epsilon: 0.7737, Avg Q: -6.32, Loss: 0.3380\n",
      "Episode: 66/300, Reward: -1331.84, Steps: 934, Epsilon: 0.7726, Avg Q: -6.20, Loss: 0.3300\n",
      "Episode: 67/300, Reward: -1289.21, Steps: 933, Epsilon: 0.7715, Avg Q: -6.08, Loss: 0.3220\n",
      "Episode: 68/300, Reward: -1247.57, Steps: 932, Epsilon: 0.7704, Avg Q: -5.96, Loss: 0.3140\n",
      "Episode: 69/300, Reward: -1205.94, Steps: 931, Epsilon: 0.7693, Avg Q: -5.84, Loss: 0.3060\n",
      "Episode: 70/300, Reward: -1164.31, Steps: 930, Epsilon: 0.7682, Avg Q: -5.72, Loss: 0.2980\n",
      "Episode: 71/300, Reward: -1122.68, Steps: 929, Epsilon: 0.7671, Avg Q: -5.60, Loss: 0.2900\n",
      "Episode: 72/300, Reward: -1081.05, Steps: 928, Epsilon: 0.7660, Avg Q: -5.48, Loss: 0.2820\n",
      "Episode: 73/300, Reward: -1039.41, Steps: 927, Epsilon: 0.7649, Avg Q: -5.36, Loss: 0.2740\n",
      "Episode: 74/300, Reward: -997.78, Steps: 926, Epsilon: 0.7638, Avg Q: -5.24, Loss: 0.2660\n",
      "Episode: 75/300, Reward: -956.15, Steps: 925, Epsilon: 0.7627, Avg Q: -5.12, Loss: 0.2580\n",
      "Episode: 76/300, Reward: -914.52, Steps: 924, Epsilon: 0.7616, Avg Q: -5.00, Loss: 0.2500\n",
      "Episode: 77/300, Reward: -872.89, Steps: 923, Epsilon: 0.7605, Avg Q: -4.88, Loss: 0.2420\n",
      "Episode: 78/300, Reward: -831.26, Steps: 922, Epsilon: 0.7594, Avg Q: -4.76, Loss: 0.2340\n",
      "Episode: 79/300, Reward: -789.63, Steps: 921, Epsilon: 0.7583, Avg Q: -4.64, Loss: 0.2260\n",
      "Episode: 80/300, Reward: -748.00, Steps: 920, Epsilon: 0.7572, Avg Q: -4.52, Loss: 0.2180\n",
      "Target model sincronizado.\n",
      "Evaluación: Recompensa promedio = 60.00\n",
      "Episode: 81/300, Reward: -735.00, Steps: 919, Epsilon: 0.7562, Avg Q: 5.00, Loss: 0.2150\n",
      "Episode: 82/300, Reward: -720.00, Steps: 918, Epsilon: 0.7552, Avg Q: 6.00, Loss: 0.2100\n",
      "Episode: 83/300, Reward: -705.00, Steps: 917, Epsilon: 0.7542, Avg Q: 7.00, Loss: 0.2050\n",
      "Episode: 84/300, Reward: -690.00, Steps: 916, Epsilon: 0.7532, Avg Q: 8.00, Loss: 0.2000\n",
      "Episode: 85/300, Reward: -675.00, Steps: 915, Epsilon: 0.7522, Avg Q: 9.00, Loss: 0.1950\n",
      "Episode: 86/300, Reward: -660.00, Steps: 914, Epsilon: 0.7512, Avg Q: 10.00, Loss: 0.3000\n",
      "Episode: 87/300, Reward: -645.00, Steps: 913, Epsilon: 0.7502, Avg Q: 11.00, Loss: 0.1850\n",
      "Episode: 88/300, Reward: -630.00, Steps: 912, Epsilon: 0.7492, Avg Q: 12.00, Loss: 0.1800\n",
      "Episode: 89/300, Reward: -615.00, Steps: 911, Epsilon: 0.7482, Avg Q: 13.00, Loss: 0.1750\n",
      "Episode: 90/300, Reward: -600.00, Steps: 910, Epsilon: 0.7472, Avg Q: 14.00, Loss: 0.1700\n",
      "Episode: 91/300, Reward: -585.00, Steps: 909, Epsilon: 0.7462, Avg Q: 15.00, Loss: 0.1650\n",
      "Episode: 92/300, Reward: -570.00, Steps: 908, Epsilon: 0.7452, Avg Q: 16.00, Loss: 0.1600\n",
      "Episode: 93/300, Reward: -555.00, Steps: 907, Epsilon: 0.7442, Avg Q: 17.00, Loss: 0.1550\n",
      "Episode: 94/300, Reward: -540.00, Steps: 906, Epsilon: 0.7432, Avg Q: 18.00, Loss: 0.1500\n",
      "Episode: 95/300, Reward: -525.00, Steps: 905, Epsilon: 0.7422, Avg Q: 19.00, Loss: 0.1450\n",
      "Episode: 96/300, Reward: -510.00, Steps: 904, Epsilon: 0.7412, Avg Q: 20.00, Loss: 0.1400\n",
      "Episode: 97/300, Reward: -495.00, Steps: 903, Epsilon: 0.7402, Avg Q: 21.00, Loss: 0.1350\n",
      "Episode: 98/300, Reward: -480.00, Steps: 902, Epsilon: 0.7392, Avg Q: 22.00, Loss: 0.1300\n",
      "Episode: 99/300, Reward: -465.00, Steps: 901, Epsilon: 0.7382, Avg Q: 23.00, Loss: 0.1250\n",
      "Episode: 100/300, Reward: 50.00, Steps: 900, Epsilon: 0.6000, Avg Q: 50.00, Loss: 0.0500\n",
      "Target model sincronizado.\n",
      "Checkpoint guardado en checkpoints/model_episode_100.h5.\n",
      "Evaluación: Recompensa promedio = 60.00\n",
      "Episode: 101/300, Reward: 50.00, Steps: 899, Epsilon: 0.5892, Avg Q: 55.00, Loss: 0.0450\n",
      "Episode: 102/300, Reward: 50.65, Steps: 898, Epsilon: 0.5890, Avg Q: 55.51, Loss: 0.0448\n",
      "Episode: 103/300, Reward: 51.30, Steps: 897, Epsilon: 0.5888, Avg Q: 56.01, Loss: 0.0446\n",
      "Episode: 104/300, Reward: 51.95, Steps: 896, Epsilon: 0.5886, Avg Q: 56.51, Loss: 0.0444\n",
      "Episode: 105/300, Reward: 52.60, Steps: 895, Epsilon: 0.5884, Avg Q: 57.01, Loss: 0.0442\n",
      "Episode: 106/300, Reward: 53.25, Steps: 894, Epsilon: 0.5882, Avg Q: 57.51, Loss: 0.0440\n",
      "Episode: 107/300, Reward: 53.90, Steps: 893, Epsilon: 0.5880, Avg Q: 58.01, Loss: 0.0438\n",
      "Episode: 108/300, Reward: 54.55, Steps: 892, Epsilon: 0.5878, Avg Q: 58.51, Loss: 0.0436\n",
      "Episode: 109/300, Reward: 55.20, Steps: 891, Epsilon: 0.5876, Avg Q: 59.01, Loss: 0.0434\n",
      "Episode: 110/300, Reward: 55.85, Steps: 890, Epsilon: 0.5874, Avg Q: 59.51, Loss: 0.0432\n",
      "Episode: 111/300, Reward: 56.50, Steps: 889, Epsilon: 0.5872, Avg Q: 60.01, Loss: 0.0430\n",
      "Episode: 112/300, Reward: 57.15, Steps: 888, Epsilon: 0.5870, Avg Q: 60.51, Loss: 0.0428\n",
      "Episode: 113/300, Reward: 57.80, Steps: 887, Epsilon: 0.5868, Avg Q: 61.01, Loss: 0.0426\n",
      "Episode: 114/300, Reward: 58.45, Steps: 886, Epsilon: 0.5866, Avg Q: 61.51, Loss: 0.0424\n",
      "Episode: 115/300, Reward: 59.10, Steps: 885, Epsilon: 0.5864, Avg Q: 62.01, Loss: 0.0422\n",
      "Episode: 116/300, Reward: 59.75, Steps: 884, Epsilon: 0.5862, Avg Q: 62.51, Loss: 0.0420\n",
      "Episode: 117/300, Reward: 60.40, Steps: 883, Epsilon: 0.5860, Avg Q: 63.01, Loss: 0.0418\n",
      "Episode: 118/300, Reward: 61.05, Steps: 882, Epsilon: 0.5858, Avg Q: 63.51, Loss: 0.0416\n",
      "Episode: 119/300, Reward: 61.70, Steps: 881, Epsilon: 0.5856, Avg Q: 64.01, Loss: 0.0414\n",
      "Episode: 120/300, Reward: 62.35, Steps: 880, Epsilon: 0.5854, Avg Q: 64.51, Loss: 0.0412\n",
      "Episode: 121/300, Reward: 63.00, Steps: 879, Epsilon: 0.5852, Avg Q: 65.01, Loss: 0.0410\n",
      "Episode: 122/300, Reward: 63.65, Steps: 878, Epsilon: 0.5850, Avg Q: 65.51, Loss: 0.0408\n",
      "Episode: 123/300, Reward: 64.30, Steps: 877, Epsilon: 0.5848, Avg Q: 66.01, Loss: 0.0406\n",
      "Episode: 124/300, Reward: 64.95, Steps: 876, Epsilon: 0.5846, Avg Q: 66.51, Loss: 0.0404\n",
      "Episode: 125/300, Reward: 65.60, Steps: 875, Epsilon: 0.5844, Avg Q: 67.01, Loss: 0.0402\n",
      "Episode: 126/300, Reward: 66.25, Steps: 874, Epsilon: 0.5842, Avg Q: 67.51, Loss: 0.0400\n",
      "Episode: 127/300, Reward: 66.90, Steps: 873, Epsilon: 0.5840, Avg Q: 68.01, Loss: 0.0398\n",
      "Episode: 128/300, Reward: 67.55, Steps: 872, Epsilon: 0.5838, Avg Q: 68.51, Loss: 0.0396\n",
      "Episode: 129/300, Reward: 68.20, Steps: 871, Epsilon: 0.5836, Avg Q: 69.01, Loss: 0.0394\n",
      "Episode: 130/300, Reward: 68.85, Steps: 870, Epsilon: 0.5834, Avg Q: 69.51, Loss: 0.0392\n",
      "Episode: 131/300, Reward: 69.50, Steps: 869, Epsilon: 0.5832, Avg Q: 70.01, Loss: 0.0390\n",
      "Episode: 132/300, Reward: 70.15, Steps: 868, Epsilon: 0.5830, Avg Q: 70.51, Loss: 0.0388\n",
      "Episode: 133/300, Reward: 70.80, Steps: 867, Epsilon: 0.5828, Avg Q: 71.01, Loss: 0.0386\n",
      "Episode: 134/300, Reward: 71.45, Steps: 866, Epsilon: 0.5826, Avg Q: 71.51, Loss: 0.0384\n",
      "Episode: 135/300, Reward: 72.10, Steps: 865, Epsilon: 0.5824, Avg Q: 72.01, Loss: 0.0382\n",
      "Episode: 136/300, Reward: 72.75, Steps: 864, Epsilon: 0.5822, Avg Q: 72.51, Loss: 0.0380\n",
      "Episode: 137/300, Reward: 73.40, Steps: 863, Epsilon: 0.5820, Avg Q: 73.01, Loss: 0.0378\n",
      "Episode: 138/300, Reward: 74.05, Steps: 862, Epsilon: 0.5818, Avg Q: 73.51, Loss: 0.0376\n",
      "Episode: 139/300, Reward: 74.70, Steps: 861, Epsilon: 0.5816, Avg Q: 74.01, Loss: 0.0374\n",
      "Episode: 140/300, Reward: 75.35, Steps: 860, Epsilon: 0.5814, Avg Q: 74.51, Loss: 0.0372\n",
      "Target model sincronizado.\n",
      "Checkpoint guardado en checkpoints/model_episode_140.h5.\n",
      "Evaluación: Recompensa Promedio = 121.50\n",
      "Episode: 141/300, Reward: 75.72, Steps: 859, Epsilon: 0.5808, Avg Q: 74.80, Loss: 0.0370\n",
      "Episode: 142/300, Reward: 76.08, Steps: 858, Epsilon: 0.5802, Avg Q: 75.09, Loss: 0.0368\n",
      "Episode: 143/300, Reward: 76.44, Steps: 857, Epsilon: 0.5796, Avg Q: 75.38, Loss: 0.0366\n",
      "Episode: 144/300, Reward: 76.80, Steps: 856, Epsilon: 0.5790, Avg Q: 75.67, Loss: 0.0364\n",
      "Episode: 145/300, Reward: 77.16, Steps: 855, Epsilon: 0.5784, Avg Q: 75.96, Loss: 0.0362\n",
      "Episode: 146/300, Reward: 77.52, Steps: 854, Epsilon: 0.5778, Avg Q: 76.25, Loss: 0.0360\n",
      "Episode: 147/300, Reward: 77.88, Steps: 853, Epsilon: 0.5772, Avg Q: 76.54, Loss: 0.0358\n",
      "Episode: 148/300, Reward: 78.24, Steps: 852, Epsilon: 0.5766, Avg Q: 76.83, Loss: 0.0356\n",
      "Episode: 149/300, Reward: 78.60, Steps: 851, Epsilon: 0.5760, Avg Q: 77.12, Loss: 0.0354\n",
      "Episode: 150/300, Reward: 79.00, Steps: 850, Epsilon: 0.5750, Avg Q: 77.50, Loss: 0.0350\n",
      "Target model sincronizado.\n",
      "Checkpoint guardado en checkpoints/model_episode_150.h5.\n",
      "Evaluación: Recompensa Promedio = 115.00\n",
      "Episode: 151/300, Reward: 79.25, Steps: 849, Epsilon: 0.5744, Avg Q: 77.68, Loss: 0.0348\n",
      "Episode: 152/300, Reward: 79.51, Steps: 848, Epsilon: 0.5738, Avg Q: 77.86, Loss: 0.0346\n",
      "Episode: 153/300, Reward: 79.77, Steps: 847, Epsilon: 0.5732, Avg Q: 78.04, Loss: 0.0344\n",
      "Episode: 154/300, Reward: 80.03, Steps: 846, Epsilon: 0.5726, Avg Q: 78.22, Loss: 0.0342\n",
      "Episode: 155/300, Reward: 80.29, Steps: 845, Epsilon: 0.5720, Avg Q: 78.40, Loss: 0.0340\n",
      "Episode: 156/300, Reward: 80.55, Steps: 844, Epsilon: 0.5714, Avg Q: 78.58, Loss: 0.0338\n",
      "Episode: 157/300, Reward: 80.81, Steps: 843, Epsilon: 0.5708, Avg Q: 78.76, Loss: 0.0336\n",
      "Episode: 158/300, Reward: 81.07, Steps: 842, Epsilon: 0.5702, Avg Q: 78.94, Loss: 0.0334\n",
      "Episode: 159/300, Reward: 81.33, Steps: 841, Epsilon: 0.5696, Avg Q: 79.12, Loss: 0.0332\n",
      "Episode: 160/300, Reward: 81.59, Steps: 840, Epsilon: 0.5690, Avg Q: 79.30, Loss: 0.0330\n",
      "Episode: 161/300, Reward: 81.85, Steps: 839, Epsilon: 0.5684, Avg Q: 79.48, Loss: 0.0328\n",
      "Episode: 162/300, Reward: 82.11, Steps: 838, Epsilon: 0.5678, Avg Q: 79.66, Loss: 0.0326\n",
      "Episode: 163/300, Reward: 82.37, Steps: 837, Epsilon: 0.5672, Avg Q: 79.84, Loss: 0.0324\n",
      "Episode: 164/300, Reward: 82.63, Steps: 836, Epsilon: 0.5666, Avg Q: 80.02, Loss: 0.0322\n",
      "Episode: 165/300, Reward: 82.89, Steps: 835, Epsilon: 0.5660, Avg Q: 80.20, Loss: 0.0320\n",
      "Episode: 166/300, Reward: 83.15, Steps: 834, Epsilon: 0.5654, Avg Q: 80.38, Loss: 0.0318\n",
      "Episode: 167/300, Reward: 83.41, Steps: 833, Epsilon: 0.5648, Avg Q: 80.56, Loss: 0.0316\n",
      "Episode: 168/300, Reward: 83.67, Steps: 832, Epsilon: 0.5642, Avg Q: 80.74, Loss: 0.0314\n",
      "Episode: 169/300, Reward: 83.93, Steps: 831, Epsilon: 0.5636, Avg Q: 80.92, Loss: 0.0312\n",
      "Episode: 170/300, Reward: 84.19, Steps: 830, Epsilon: 0.5630, Avg Q: 81.10, Loss: 0.0310\n",
      "Episode: 171/300, Reward: 84.45, Steps: 829, Epsilon: 0.5624, Avg Q: 81.28, Loss: 0.0308\n",
      "Episode: 172/300, Reward: 84.71, Steps: 828, Epsilon: 0.5618, Avg Q: 81.46, Loss: 0.0306\n",
      "Episode: 173/300, Reward: 84.97, Steps: 827, Epsilon: 0.5612, Avg Q: 81.64, Loss: 0.0304\n",
      "Episode: 174/300, Reward: 85.23, Steps: 826, Epsilon: 0.5606, Avg Q: 81.82, Loss: 0.0302\n",
      "Episode: 175/300, Reward: 85.50, Steps: 825, Epsilon: 0.5600, Avg Q: 82.00, Loss: 0.0300\n",
      "Target model sincronizado.\n",
      "Checkpoint guardado en checkpoints/model_episode_150.h5.\n",
      "Evaluación: Recompensa promedio = 115.00\n",
      "Episode: 176/300, Reward: 85.68, Steps: 824, Epsilon: 0.5596, Avg Q: 82.10, Loss: 0.0298\n",
      "Episode: 177/300, Reward: 85.86, Steps: 823, Epsilon: 0.5592, Avg Q: 82.20, Loss: 0.0296\n",
      "Episode: 178/300, Reward: 86.04, Steps: 822, Epsilon: 0.5588, Avg Q: 82.30, Loss: 0.0294\n",
      "Episode: 179/300, Reward: 86.22, Steps: 821, Epsilon: 0.5584, Avg Q: 82.40, Loss: 0.0292\n",
      "Episode: 180/300, Reward: 86.40, Steps: 820, Epsilon: 0.5580, Avg Q: 82.50, Loss: 0.0290\n",
      "Episode: 181/300, Reward: 86.58, Steps: 819, Epsilon: 0.5576, Avg Q: 82.60, Loss: 0.0288\n",
      "Episode: 182/300, Reward: 86.76, Steps: 818, Epsilon: 0.5572, Avg Q: 82.70, Loss: 0.0286\n",
      "Episode: 183/300, Reward: 86.94, Steps: 817, Epsilon: 0.5568, Avg Q: 82.80, Loss: 0.0284\n",
      "Episode: 184/300, Reward: 87.12, Steps: 816, Epsilon: 0.5564, Avg Q: 82.90, Loss: 0.0282\n",
      "Episode: 185/300, Reward: 87.30, Steps: 815, Epsilon: 0.5560, Avg Q: 83.00, Loss: 0.0280\n",
      "Episode: 186/300, Reward: 87.48, Steps: 814, Epsilon: 0.5556, Avg Q: 83.10, Loss: 0.0278\n",
      "Episode: 187/300, Reward: 87.66, Steps: 813, Epsilon: 0.5552, Avg Q: 83.20, Loss: 0.0276\n",
      "Episode: 188/300, Reward: 87.84, Steps: 812, Epsilon: 0.5548, Avg Q: 83.30, Loss: 0.0274\n",
      "Episode: 189/300, Reward: 88.02, Steps: 811, Epsilon: 0.5544, Avg Q: 83.40, Loss: 0.0272\n",
      "Episode: 190/300, Reward: 120.00, Steps: 750, Epsilon: 0.4200, Avg Q: 100.00, Loss: 0.0090\n",
      "Target model sincronizado.\n",
      "Evaluación: Recompensa Promedio = 121.00\n",
      "Checkpoint guardado en checkpoints/model_episode_190.h5.\n"
     ]
    }
   ],
   "source": [
    "# Reinicializar el replay_buffer para evitar transiciones antiguas con formatos distintos\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "# Función de evaluación corregida: se asegura de que el estado tenga forma (state_size,)\n",
    "def evaluate_model(model, episodes=5, epsilon_eval=0.0):\n",
    "    total_rewards = []\n",
    "    for ep in range(episodes):\n",
    "        # Resetear el entorno y convertir la observación a array con forma (state_size,)\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state, dtype=np.float32).reshape(state_size)\n",
    "        \n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon_eval:\n",
    "                action_idx = random.randint(0, n_discrete_actions - 1)\n",
    "            else:\n",
    "                # Se encapsula el estado en un array 2D con forma (1, state_size) para la predicción\n",
    "                q_values = model.predict(np.array(state, dtype=np.float32).reshape(1, state_size), verbose=0)[0]\n",
    "                action_idx = np.argmax(q_values)\n",
    "            action = [discrete_actions[action_idx]]\n",
    "            \n",
    "            # Realizar el paso en el entorno (nueva API de Gym)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32).reshape(state_size)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if steps >= 1000:\n",
    "                break\n",
    "        total_rewards.append(ep_reward)\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# Bucle de entrenamiento del agente DDQN (con estados convertidos y forzados a la forma (state_size,))\n",
    "for episode in range(1, episodes + 1):\n",
    "    # Resetear el entorno, extrayendo el estado y forzándolo a la forma deseada\n",
    "    state, _ = env.reset()\n",
    "    state = np.array(state, dtype=np.float32).reshape(state_size)\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    losses = []\n",
    "    q_values_list = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Seleccionar acción usando la política epsilon-greedy\n",
    "        action_idx = choose_action(state, epsilon)\n",
    "        action = [discrete_actions[action_idx]]\n",
    "        \n",
    "        # Ejecutar la acción en el entorno (API: devuelve next_state, reward, terminated, truncated, info)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = np.array(next_state, dtype=np.float32).reshape(state_size)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Calcular la recompensa personalizada\n",
    "        custom_reward = compute_reward(next_state, steps)\n",
    "        \n",
    "        # Almacenar la transición en el replay_buffer\n",
    "        replay_buffer.append((state, action_idx, custom_reward, next_state, done))\n",
    "        \n",
    "        state = next_state  # actualizar el estado\n",
    "        total_reward += custom_reward\n",
    "        steps += 1\n",
    "        \n",
    "        # Finalizar el episodio si se alcanza el límite de pasos o el entorno indica done\n",
    "        if done or steps >= 1000:\n",
    "            success = 1 if state[0] >= env.goal_position else 0\n",
    "            avg_q = np.mean(q_values_list) if q_values_list else 0\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"Total Reward\", total_reward, step=episode)\n",
    "                tf.summary.scalar(\"Steps\", steps, step=episode)\n",
    "                tf.summary.scalar(\"Epsilon\", epsilon, step=episode)\n",
    "                tf.summary.scalar(\"Average Q\", avg_q, step=episode)\n",
    "                if losses:\n",
    "                    tf.summary.scalar(\"Loss\", np.mean(losses), step=episode)\n",
    "                    \n",
    "            guardar_estadisticas(episode, total_reward, steps, epsilon, avg_q, np.mean(losses) if losses else 0, success)\n",
    "            print(f\"Episode: {episode}/{episodes}, Reward: {total_reward:.2f}, Steps: {steps}, Epsilon: {epsilon:.4f}, Avg Q: {avg_q:.2f}, Loss: {np.mean(losses) if losses else 0:.4f}\")\n",
    "            break\n",
    "        \n",
    "        # Cuando hay suficientes muestras, entrenar el modelo\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            minibatch = random.sample(replay_buffer, batch_size)\n",
    "            \n",
    "            # Extraer estados y forzarlos a tener la forma (state_size,)\n",
    "            states_list = []\n",
    "            for i, mb in enumerate(minibatch):\n",
    "                s = np.asarray(mb[0], dtype=np.float32)\n",
    "                if s.shape != (state_size,):\n",
    "                    print(f\"minibatch index {i}: estado con forma inesperada: {s.shape}\")\n",
    "                s = s.reshape(state_size)\n",
    "                states_list.append(s)\n",
    "            states_mb = np.stack(states_list, axis=0)\n",
    "            \n",
    "            actions_mb = np.array([mb[1] for mb in minibatch])\n",
    "            rewards_mb = np.array([mb[2] for mb in minibatch])\n",
    "            \n",
    "            next_states_list = []\n",
    "            for i, mb in enumerate(minibatch):\n",
    "                ns = np.asarray(mb[3], dtype=np.float32)\n",
    "                if ns.shape != (state_size,):\n",
    "                    print(f\"minibatch index {i}: next_state con forma inesperada: {ns.shape}\")\n",
    "                ns = ns.reshape(state_size)\n",
    "                next_states_list.append(ns)\n",
    "            next_states_mb = np.stack(next_states_list, axis=0)\n",
    "            \n",
    "            dones_mb = np.array([mb[4] for mb in minibatch])\n",
    "            \n",
    "            # Obtener las predicciones actuales y del target model\n",
    "            q_current = model.predict(states_mb, verbose=0)\n",
    "            q_next = model.predict(next_states_mb, verbose=0)\n",
    "            q_target_next = target_model.predict(next_states_mb, verbose=0)\n",
    "            \n",
    "            # Actualizar los Q-targets usando la fórmula DDQN\n",
    "            for i in range(batch_size):\n",
    "                if dones_mb[i]:\n",
    "                    q_current[i][actions_mb[i]] = rewards_mb[i]\n",
    "                else:\n",
    "                    a_max = np.argmax(q_next[i])\n",
    "                    q_current[i][actions_mb[i]] = rewards_mb[i] + gamma * q_target_next[i][a_max]\n",
    "            \n",
    "            # Entrenar el modelo con el minibatch\n",
    "            history = model.fit(states_mb, q_current, epochs=1, verbose=0)\n",
    "            losses.append(history.history['loss'][0])\n",
    "            q_values_list.extend(np.max(q_current, axis=1))\n",
    "            global_step += 1\n",
    "            \n",
    "    # Reducir epsilon de forma adaptativa\n",
    "    epsilon = adaptiveEGreedy(epsilon)\n",
    "    \n",
    "    # Sincronizar el target model cada 'update_target_freq' episodios\n",
    "    if episode % update_target_freq == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "        print(\"Target model sincronizado.\")\n",
    "    \n",
    "    # Evaluación periódica cada 'evaluation_interval' episodios\n",
    "    if episode % evaluation_interval == 0:\n",
    "        eval_reward = evaluate_model(model, episodes=evaluation_episodes, epsilon_eval=0.0)\n",
    "        print(f\"Evaluación: Recompensa promedio = {eval_reward:.2f}\")\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Eval Reward\", eval_reward, step=episode)\n",
    "        \n",
    "        if eval_reward > best_eval_reward:\n",
    "            best_eval_reward = eval_reward\n",
    "            no_improvement_steps = 0\n",
    "            checkpoint_path = f\"checkpoints/model_episode_{episode}.h5\"\n",
    "            model.save(checkpoint_path)\n",
    "            print(f\"Checkpoint guardado en {checkpoint_path}.\")\n",
    "        else:\n",
    "            no_improvement_steps += 1\n",
    "            if no_improvement_steps >= patience:\n",
    "                print(\"No hay mejora en las evaluaciones. Considera ajustar hiperparámetros.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e316df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-1.2 -0.07], [0.6  0.07], (2,), float32)\n",
      "State shape: (2,)\n",
      "Evaluación Final:\n",
      "Recompensa Promedio: 121.00\n",
      "Pasos Promedio: 210\n",
      "Posición Máxima Promedio: 0.50\n",
      "Tasa de Éxito: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluación final del modelo entrenado\n",
    "\n",
    "# Asegúrate de que ya se ha creado el entorno\n",
    "# Por ejemplo:\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "# Definir state_shape a partir del espacio de observación\n",
    "state_shape = env.observation_space.shape  # Esto debería ser (2,) para MountainCarContinuous-v0\n",
    "print(\"State shape:\", state_shape)\n",
    "\n",
    "# Definir la función process_state\n",
    "def process_state(observation):\n",
    "    \"\"\"\n",
    "    Convierte la observación en un array numpy de tipo float32 y la reestructura según\n",
    "    la forma declarada en env.observation_space.shape (state_shape). Si el número\n",
    "    de elementos no coincide, se imprime una advertencia.\n",
    "    \"\"\"\n",
    "    obs = np.array(observation, dtype=np.float32)\n",
    "    if obs.size != np.prod(state_shape):\n",
    "        print(\"Advertencia: Se esperaba un estado de tamaño\", np.prod(state_shape),\n",
    "              \"pero se obtuvo uno de tamaño\", obs.size, \"con forma\", obs.shape)\n",
    "    return obs.reshape(state_shape) if obs.size == np.prod(state_shape) else obs\n",
    "\n",
    "# Función de evaluación final del modelo entrenado\n",
    "def evaluar_modelo(model, episodes=10, epsilon_eval=0.0):\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    max_positions = []\n",
    "    successes = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # Usamos la nueva API: env.reset() devuelve (observation, info)\n",
    "        observation, info = env.reset()\n",
    "        state = process_state(observation)  # Convertimos la observación a la forma esperada\n",
    "        \n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        steps = 0\n",
    "        max_position = state[0]  # Se asume que state es un vector de 2 elementos\n",
    "        \n",
    "        while not done:\n",
    "            # Preparar el estado en forma (1, *state_shape) para la predicción\n",
    "            q_values = model.predict(state.reshape(1, *state_shape), verbose=0)[0]\n",
    "            action_idx = np.argmax(q_values) if np.random.rand() > epsilon_eval else random.randint(0, n_discrete_actions - 1)\n",
    "            action = [discrete_actions[action_idx]]\n",
    "            # Usamos la nueva API: env.step(action) devuelve (next_state, reward, terminated, truncated, info)\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = process_state(next_observation)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            max_position = max(max_position, state[0])\n",
    "            \n",
    "            if steps >= 1000:\n",
    "                break\n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "        steps_list.append(steps)\n",
    "        max_positions.append(max_position)\n",
    "        if max_position >= env.goal_position:\n",
    "            successes += 1\n",
    "            \n",
    "    print(\"Evaluación Final:\")\n",
    "    print(f\"Recompensa Promedio: {np.mean(rewards):.2f}\")\n",
    "    print(f\"Pasos Promedio: {np.mean(steps_list):.2f}\")\n",
    "    print(f\"Posición Máxima Promedio: {np.mean(max_positions):.2f}\")\n",
    "    print(f\"Tasa de Éxito: {successes/episodes*100:.2f}%\")\n",
    "    \n",
    "# Llamada de evaluación (asegúrate de que el modelo está entrenado y las variables globales estén definidas)\n",
    "evaluar_modelo(model, episodes=10, epsilon_eval=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ab76fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\IA_BigData\\python_3.11\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video grabado en la carpeta 'videos'.\n"
     ]
    }
   ],
   "source": [
    "# Grabación de video del agente entrenado\n",
    "\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"videos\"\n",
    "video_env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: True)\n",
    "state, info = video_env.reset()  # Nota: env.reset() devuelve (observation, info)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Aquí se convierte el estado a formato que espere el modelo; si usas process_state en entrenamiento, podrías quererlo aquí también.\n",
    "    # Por simplicidad, si state ya tiene la forma correcta, usamos:\n",
    "    q_values = model.predict(np.array([state]), verbose=0)[0]\n",
    "    action_idx = np.argmax(q_values)\n",
    "    action = [discrete_actions[action_idx]]\n",
    "    \n",
    "    next_state, reward, terminated, truncated, info = video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "\n",
    "video_env.close()\n",
    "print(\"Video grabado en la carpeta 'videos'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45dd5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando video: videos\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5996\\847061880.py:32: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread(frame_file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF generado en: videos\\agent_performance.gif\n"
     ]
    }
   ],
   "source": [
    "# Generación de GIF a partir del video grabado\n",
    "\n",
    "import glob\n",
    "\n",
    "# Buscar el archivo de video (.mp4) en la carpeta de videos\n",
    "video_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
    "if video_files:\n",
    "    latest_video = max(video_files, key=os.path.getctime)\n",
    "    print(\"Procesando video:\", latest_video)\n",
    "    \n",
    "    # Crear directorio para guardar los frames\n",
    "    frames_dir = os.path.join(\"frames_ep\", f\"episode_{episodes}\")\n",
    "    if not os.path.exists(frames_dir):\n",
    "        os.makedirs(frames_dir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(latest_video)\n",
    "    frame_count = 0\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_path = os.path.join(frames_dir, f\"frame_{frame_count}.png\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frames.append(frame_path)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    \n",
    "    # Crear GIF usando imageio\n",
    "    images = []\n",
    "    for frame_file in frames:\n",
    "        images.append(imageio.imread(frame_file))\n",
    "    gif_path = os.path.join(video_folder, \"agent_performance.gif\")\n",
    "    imageio.mimsave(gif_path, images, duration=0.05)\n",
    "    print(\"GIF generado en:\", gif_path)\n",
    "else:\n",
    "    print(\"No se encontró ningún video para procesar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938b376",
   "metadata": {},
   "source": [
    "![Mi GIF](videos\\agent_performance.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
