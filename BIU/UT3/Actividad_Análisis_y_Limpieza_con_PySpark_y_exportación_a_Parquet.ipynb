{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjgr/IA_BigData/blob/main/BIU/UT3/Actividad_An%C3%A1lisis_y_Limpieza_con_PySpark_y_exportaci%C3%B3n_a_Parquet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzGSuGD4Agbq"
      },
      "source": [
        "#  Taller: Limpieza y Validación de Datos Complejos en Big Data con PySpark\n",
        "### Google Colab Notebook\n",
        "---\n",
        "**Objetivo:** Ejecutar un flujo completo de limpieza y validación de datos usando PySpark, detectando errores comunes y avanzados en datasets simulados, generando un archivo limpio en formato Parquet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUcIoyLxAgbv"
      },
      "source": [
        "##  1. Instalación y Configuración del Entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z14NZWpwAgbv"
      },
      "outputs": [],
      "source": [
        "# Instalación del JDK (Java Development Kit) versión 11, requerido para ejecutar Apache Spark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Descarga de Apache Spark versión 3.5.5 precompilado con soporte para Hadoop 3 desde el sitio oficial de Apache\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
        "\n",
        "# Descompresión del archivo descargado para disponer del entorno de Spark localmente\n",
        "!tar xf spark-3.5.5-bin-hadoop3.tgz\n",
        "\n",
        "# Instalación de bibliotecas necesarias para trabajar con Spark y análisis de datos\n",
        "# - pyspark: interfaz de Apache Spark para Python\n",
        "# - findspark: utilidad para encontrar e inicializar Spark en notebooks\n",
        "# - pandas y numpy: bibliotecas para análisis y estructuras de datos eficientes\n",
        "!pip install -q pyspark findspark pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XQoReWN3Agbx"
      },
      "outputs": [],
      "source": [
        "# Importación de módulos necesarios\n",
        "import os              # Para manipular variables de entorno del sistema\n",
        "import findspark       # Para inicializar el entorno de Apache Spark en notebooks\n",
        "\n",
        "# Configuración de las variables de entorno requeridas por Apache Spark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"  # Ruta al JDK 11\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\"   # Ruta a la instalación local de Spark\n",
        "\n",
        "# Inicialización del entorno de Spark con findspark\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaVGLW98Agbx"
      },
      "source": [
        "##  2. Generación Automática del Dataset Complejo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "isa-ZUSJAgbx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Establecer semilla para reproducibilidad\n",
        "np.random.seed(42)\n",
        "n_registros = 500  # Tamaño del conjunto de datos\n",
        "\n",
        "# Generación de columnas simuladas\n",
        "order_ids = np.arange(1000, 1000 + n_registros)\n",
        "\n",
        "# IDs de clientes con formato alfanumérico\n",
        "customer_ids = np.random.choice(['CUST-' + str(np.random.randint(1000, 9999)) for _ in range(100)], n_registros)\n",
        "\n",
        "# IDs de productos con formato SKU (incluye letras)\n",
        "product_ids = np.random.choice(['PROD-' + str(np.random.randint(10, 99)) + chr(np.random.randint(65, 90)) for _ in range(50)], n_registros)\n",
        "\n",
        "# Cantidades compradas, con un 10% de valores faltantes\n",
        "quantities = np.random.choice([1, 2, 3, np.nan], n_registros, p=[0.3, 0.3, 0.3, 0.1])\n",
        "\n",
        "# Precios unitarios realistas\n",
        "unit_prices = np.round(np.random.uniform(10, 300, n_registros), 2)\n",
        "\n",
        "# Introducción de valores atípicos (precios negativos o extremos)\n",
        "outlier_indices = np.random.choice(n_registros, size=10, replace=False)\n",
        "unit_prices[outlier_indices] = np.random.choice([-50, 1200], size=10)\n",
        "\n",
        "# Fechas en formatos ISO y europeo mezclados\n",
        "dates_iso = pd.date_range('2025-03-01', periods=n_registros//2).strftime('%Y-%m-%d').tolist()\n",
        "dates_euro = pd.date_range('2025-03-01', periods=n_registros//2).strftime('%d/%m/%Y').tolist()\n",
        "order_dates = np.random.choice(dates_iso + dates_euro, n_registros)\n",
        "\n",
        "# Ubicaciones de tiendas con variaciones tipográficas y valores faltantes\n",
        "locations = ['Madrid-Centro', 'Barcelona @Sants', 'Valencia  Puerto', 'sevilla-triana', 'BILBAO-Casco']\n",
        "store_locations = np.random.choice(locations + [np.nan], n_registros, p=[0.2, 0.2, 0.2, 0.2, 0.1, 0.1])\n",
        "\n",
        "# Construcción del DataFrame con todos los campos\n",
        "data = pd.DataFrame({\n",
        "    'order_id': order_ids,\n",
        "    'customer_id': customer_ids,\n",
        "    'product_id': product_ids,\n",
        "    'quantity': quantities,\n",
        "    'unit_price': unit_prices,\n",
        "    'order_date': order_dates,\n",
        "    'store_location': store_locations\n",
        "})\n",
        "\n",
        "# Inserción de duplicados (10%) y desorden aleatorio\n",
        "duplicates = data.sample(frac=0.1, random_state=42)\n",
        "data = pd.concat([data, duplicates], ignore_index=True)\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Guardado del dataset en archivo CSV\n",
        "data.to_csv('retail_sales_complex.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FyVVMx3AAgby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3580aa27-d75a-408e-f3c2-d3739250a5fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+--------+----------+----------+----------------+\n",
            "|order_id|customer_id|product_id|quantity|unit_price|order_date|  store_location|\n",
            "+--------+-----------+----------+--------+----------+----------+----------------+\n",
            "|    1195|  CUST-7863|  PROD-48A|     3.0|     17.44|11/09/2025|Barcelona @Sants|\n",
            "|    1079|  CUST-6390|  PROD-59B|     3.0|    234.31|22/05/2025|   Madrid-Centro|\n",
            "|    1480|  CUST-1769|  PROD-63V|     2.0|    188.04|25/03/2025|Valencia  Puerto|\n",
            "|    1109|  CUST-6618|  PROD-14E|     1.0|     -50.0|2025-06-10|  sevilla-triana|\n",
            "|    1280|  CUST-7873|  PROD-48A|     1.0|    127.25|22/10/2025|             nan|\n",
            "|    1440|  CUST-8916|  PROD-15L|     1.0|     85.35|2025-05-12|             nan|\n",
            "|    1084|  CUST-1860|  PROD-51I|     3.0|     31.31|2025-07-12|Valencia  Puerto|\n",
            "|    1368|  CUST-4099|  PROD-79S|     2.0|     98.36|2025-10-25|   Madrid-Centro|\n",
            "|    1132|  CUST-9792|  PROD-17P|     2.0|     205.3|28/08/2025|             nan|\n",
            "|    1364|  CUST-6618|  PROD-42F|    NULL|    264.61|17/08/2025|  sevilla-triana|\n",
            "|    1184|  CUST-8513|  PROD-34X|     3.0|    118.24|05/10/2025|Barcelona @Sants|\n",
            "|    1010|  CUST-1189|  PROD-15L|     2.0|    222.16|14/08/2025|   Madrid-Centro|\n",
            "|    1073|  CUST-8629|  PROD-54M|     1.0|     57.42|26/09/2025|  sevilla-triana|\n",
            "|    1220|  CUST-5555|  PROD-63V|     3.0|     99.11|03/08/2025|   Madrid-Centro|\n",
            "|    1278|  CUST-5297|  PROD-48A|     2.0|     49.57|2025-07-19|    BILBAO-Casco|\n",
            "|    1082|  CUST-1161|  PROD-32P|     1.0|     141.6|2025-06-23|Barcelona @Sants|\n",
            "|    1382|  CUST-2267|  PROD-48C|     3.0|    231.79|04/04/2025|Valencia  Puerto|\n",
            "|    1006|  CUST-8270|  PROD-63G|    NULL|    247.02|2025-08-03|Barcelona @Sants|\n",
            "|    1218|  CUST-1995|  PROD-84R|     2.0|     84.47|27/04/2025|             nan|\n",
            "|    1448|  CUST-9322|  PROD-63V|     2.0|    242.45|07/04/2025|   Madrid-Centro|\n",
            "|    1396|  CUST-9529|  PROD-72Q|     2.0|    156.78|2025-04-08|  sevilla-triana|\n",
            "|    1167|  CUST-9684|  PROD-71Y|     1.0|    104.84|22/08/2025|    BILBAO-Casco|\n",
            "|    1468|  CUST-3731|  PROD-70L|     1.0|    101.47|09/04/2025|    BILBAO-Casco|\n",
            "|    1172|  CUST-3731|  PROD-74V|     2.0|    136.32|2025-03-16|Valencia  Puerto|\n",
            "|    1081|  CUST-3568|  PROD-24A|     1.0|    291.25|2025-09-26|Valencia  Puerto|\n",
            "|    1077|  CUST-9792|  PROD-17P|     3.0|    200.45|13/06/2025|             nan|\n",
            "|    1185|  CUST-9154|  PROD-96Y|     1.0|     75.91|05/03/2025|Barcelona @Sants|\n",
            "|    1289|  CUST-7736|  PROD-63G|     1.0|     89.51|11/06/2025|Barcelona @Sants|\n",
            "|    1377|  CUST-4561|  PROD-13D|     1.0|     48.43|2025-07-21|Valencia  Puerto|\n",
            "|    1124|  CUST-7863|  PROD-54M|     3.0|    159.52|2025-09-22|             nan|\n",
            "|    1250|  CUST-4104|  PROD-74V|     2.0|     129.2|12/03/2025|Valencia  Puerto|\n",
            "|    1481|  CUST-1995|  PROD-94K|     1.0|     29.33|2025-08-04|   Madrid-Centro|\n",
            "|    1328|  CUST-6051|  PROD-66R|     1.0|     78.05|2025-06-16|Barcelona @Sants|\n",
            "|    1072|  CUST-8555|  PROD-54M|     1.0|    181.96|20/10/2025|   Madrid-Centro|\n",
            "|    1148|  CUST-5297|  PROD-66R|     2.0|    279.98|2025-05-28|   Madrid-Centro|\n",
            "|    1076|  CUST-1769|  PROD-26G|     1.0|    1200.0|21/07/2025|             nan|\n",
            "|    1002|  CUST-3731|  PROD-47S|     1.0|     246.3|05/04/2025|Valencia  Puerto|\n",
            "|    1165|  CUST-2267|  PROD-54M|     3.0|    264.77|2025-06-18|   Madrid-Centro|\n",
            "|    1416|  CUST-7863|  PROD-63V|    NULL|      39.3|10/05/2025|   Madrid-Centro|\n",
            "|    1428|  CUST-1860|  PROD-72Q|     2.0|    267.12|29/03/2025|             nan|\n",
            "|    1199|  CUST-8208|  PROD-48C|     3.0|    178.25|01/10/2025|  sevilla-triana|\n",
            "|    1209|  CUST-1189|  PROD-48C|     1.0|    197.77|21/05/2025|    BILBAO-Casco|\n",
            "|    1354|  CUST-8869|  PROD-12M|     1.0|    159.55|2025-08-24|             nan|\n",
            "|    1077|  CUST-9792|  PROD-17P|     3.0|    200.45|13/06/2025|             nan|\n",
            "|    1153|  CUST-9529|  PROD-47S|     1.0|    124.22|25/10/2025|    BILBAO-Casco|\n",
            "|    1386|  CUST-9529|  PROD-28T|     3.0|     52.45|26/04/2025|Barcelona @Sants|\n",
            "|    1491|  CUST-6051|  PROD-28T|     3.0|     30.66|2025-07-04|Barcelona @Sants|\n",
            "|    1068|  CUST-1161|  PROD-23E|     1.0|    262.52|03/09/2025|Valencia  Puerto|\n",
            "|    1280|  CUST-7873|  PROD-48A|     1.0|    127.25|22/10/2025|             nan|\n",
            "|    1406|  CUST-8849|  PROD-94K|     3.0|    143.34|15/07/2025|  sevilla-triana|\n",
            "+--------+-----------+----------+--------+----------+----------+----------------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicialización de la sesión de Spark para procesamiento distribuido\n",
        "spark = SparkSession.builder .appName(\"DataQualityComplex\") .getOrCreate() # Nombre de la aplicación (identificable en UI de Spark)\n",
        "\n",
        "# Carga del dataset desde archivo CSV, infiriendo tipos de datos automáticamente\n",
        "df = spark.read.csv(\"retail_sales_complex.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Visualización de las primeras 50 filas del DataFrame para inspección inicial\n",
        "df.show(50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RqfOVmrAgby"
      },
      "source": [
        "##  3. Diagnóstico de Calidad de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ttxqc6NPAgbz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e786394-0798-41bb-f602-35acc446c0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "|order_id|customer_id|product_id|quantity|unit_price|order_date|store_location|\n",
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "|       0|          0|         0|      51|         0|         0|             0|\n",
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "\n",
            "Duplicados detectados: 50\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "# Cómputo de valores nulos por columna\n",
        "# Se utiliza 'when' para marcar valores nulos y 'count' para contarlos\n",
        "df.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
        "]).show()\n",
        "\n",
        "# Cómputo de registros duplicados\n",
        "# Se calcula la diferencia entre el total de registros y los únicos\n",
        "print(f\"Duplicados detectados: {df.count() - df.dropDuplicates().count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conservación del dataset original antes de iniciar el proceso de limpieza\n",
        "# Importante para trazabilidad y comparaciones posteriores\n",
        "df_original = df\n",
        "\n",
        "# Cómputo del total de registros originales (útil para análisis de impacto de limpieza)\n",
        "total_registros = df_original.count()"
      ],
      "metadata": {
        "id": "Sk6mZQuV_siI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preservación del dataset original antes de aplicar transformaciones\n",
        "# Esto permite comparar resultados después de la limpieza o revertir cambios si es necesario\n",
        "df_original = df\n",
        "\n",
        "# Almacenamiento del número total de registros originales para evaluar impacto de la limpieza\n",
        "total_registros = df_original.count()"
      ],
      "metadata": {
        "id": "zbE-AYB5S2gw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4nd6zArAgbz"
      },
      "source": [
        "##  4. Limpieza Avanzada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tMv3k7u4Agbz"
      },
      "outputs": [],
      "source": [
        "# --- Limpieza de valores nulos ---\n",
        "\n",
        "# Cálculo de la media de la columna 'quantity' ignorando nulos\n",
        "# Se redondea el valor para mantener consistencia con datos enteros\n",
        "mean_quantity = df.agg({'quantity': 'mean'}).first()[0]\n",
        "\n",
        "# Relleno de valores nulos:\n",
        "# - 'quantity' se imputa con la media redondeada\n",
        "# - 'store_location' se reemplaza con un valor genérico 'Desconocido'\n",
        "df = df.fillna({\n",
        "    'quantity': round(mean_quantity, 0),\n",
        "    'store_location': 'Desconocido'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9gqlDhWnAgbz"
      },
      "outputs": [],
      "source": [
        "# --- Eliminación de duplicados ---\n",
        "\n",
        "# Se eliminan registros duplicados considerando como clave compuesta:\n",
        "# - order_id\n",
        "# - customer_id\n",
        "# - order_date\n",
        "# Esta combinación representa una transacción única por cliente en una fecha determinada.\n",
        "df = df.dropDuplicates(['order_id', 'customer_id', 'order_date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lYllaVnlAgb0"
      },
      "outputs": [],
      "source": [
        "# --- Normalización de fechas ---\n",
        "\n",
        "from pyspark.sql.functions import to_date, col, when\n",
        "\n",
        "# Conversión de fechas en distintos formatos al tipo DateType\n",
        "# - Si el formato es europeo (dd/MM/yyyy), se aplica dicho patrón\n",
        "# - En caso contrario, se asume formato ISO (yyyy-MM-dd)\n",
        "df = df.withColumn('order_date',\n",
        "    when(col('order_date').rlike(r'\\d{2}/\\d{2}/\\d{4}'),\n",
        "         to_date(col('order_date'), 'dd/MM/yyyy'))\n",
        "    .otherwise(to_date(col('order_date'), 'yyyy-MM-dd'))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QRKLe0ItAgb0"
      },
      "outputs": [],
      "source": [
        "# --- Limpieza y normalización de la columna 'store_location' ---\n",
        "\n",
        "from pyspark.sql.functions import regexp_replace, lower, trim, col\n",
        "\n",
        "# Reemplazo de caracteres inconsistentes o problemáticos\n",
        "# - '@' → '-' para unificar separadores\n",
        "# - Doble espacio → '-' para evitar errores en tokenización\n",
        "df = df.withColumn('store_location', regexp_replace('store_location', '@', '-'))\n",
        "df = df.withColumn('store_location', regexp_replace('store_location', '  ', '-'))\n",
        "\n",
        "# Conversión a minúsculas y eliminación de espacios sobrantes\n",
        "df = df.withColumn('store_location', lower(trim(col('store_location'))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FZOeeVhLAgb0"
      },
      "outputs": [],
      "source": [
        "# --- Tratamiento de outliers en 'unit_price' usando el método del rango intercuartílico (IQR) ---\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Cálculo aproximado de los cuartiles primero (Q1) y tercero (Q3)\n",
        "# La función approxQuantile es eficiente para grandes volúmenes de datos\n",
        "Q1 = df.approxQuantile(\"unit_price\", [0.25], 0.05)[0]\n",
        "Q3 = df.approxQuantile(\"unit_price\", [0.75], 0.05)[0]\n",
        "\n",
        "# Cálculo del rango intercuartílico (IQR)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Filtrado de valores extremos: solo se conservan aquellos dentro de 1.5 × IQR del rango intercuartílico\n",
        "df = df.filter(\n",
        "    (col(\"unit_price\") >= (Q1 - 1.5 * IQR)) &\n",
        "    (col(\"unit_price\") <= (Q3 + 1.5 * IQR))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VabRniJSAgb0"
      },
      "source": [
        "##  5. Análisis de Métricas de Calidad Pre y Post Limpieza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np1hSKL1Agb1"
      },
      "source": [
        "###  Porcentaje de Nulos, Duplicados y Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rEjpqprAAgb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277da688-2a08-4cf3-a196-08383e5ba67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------------+--------------+-----------------+--------------+--------------+------------------+\n",
            "|order_id_pct|customer_id_pct|product_id_pct|     quantity_pct|unit_price_pct|order_date_pct|store_location_pct|\n",
            "+------------+---------------+--------------+-----------------+--------------+--------------+------------------+\n",
            "|         0.0|            0.0|           0.0|9.272727272727273|           0.0|           0.0|               0.0|\n",
            "+------------+---------------+--------------+-----------------+--------------+--------------+------------------+\n",
            "\n",
            "Duplicados detectados: 50 (9.09%)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import count, when, col\n",
        "\n",
        "# --- 1. Análisis de valores nulos ---\n",
        "\n",
        "# Cálculo del número de valores nulos por columna\n",
        "nulos = df_original.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c) for c in df_original.columns\n",
        "])\n",
        "\n",
        "# Conversión de los conteos a porcentajes respecto al total de registros originales\n",
        "nulos_percent = nulos.select([\n",
        "    (col(c) / total_registros * 100).alias(c + '_pct') for c in df_original.columns\n",
        "])\n",
        "\n",
        "# Visualización del porcentaje de valores nulos por columna\n",
        "nulos_percent.show()\n",
        "\n",
        "# --- 2. Detección de duplicados ---\n",
        "\n",
        "# Cómputo de registros duplicados en el dataset original\n",
        "duplicados = total_registros - df_original.dropDuplicates().count()\n",
        "\n",
        "# Presentación de resultados con porcentaje relativo\n",
        "print(f'Duplicados detectados: {duplicados} ({round(duplicados / total_registros * 100, 2)}%)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHdqHFcUAgb1"
      },
      "source": [
        "###  Estadísticas Descriptivas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-4OkPwiqAgb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4309dcea-e960-4226-da8e-ada7e1d3394a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antes de la limpieza:\n",
            "+-------+------------------+------------------+\n",
            "|summary|          quantity|        unit_price|\n",
            "+-------+------------------+------------------+\n",
            "|  count|               499|               550|\n",
            "|   mean|1.9619238476953909|166.47359999999998|\n",
            "| stddev|0.8410604239645704|144.73499241089607|\n",
            "|    min|               1.0|             -50.0|\n",
            "|    max|               3.0|            1200.0|\n",
            "+-------+------------------+------------------+\n",
            "\n",
            "Después de la limpieza:\n",
            "+-------+------------------+------------------+\n",
            "|summary|          quantity|        unit_price|\n",
            "+-------+------------------+------------------+\n",
            "|  count|               494|               494|\n",
            "|   mean|1.9838056680161944|154.52921052631584|\n",
            "| stddev|0.8004441532568164| 85.41247319250427|\n",
            "|    min|               1.0|             -50.0|\n",
            "|    max|               3.0|            299.83|\n",
            "+-------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Comparación estadística antes y después de la limpieza ---\n",
        "\n",
        "# Estadísticas descriptivas del dataset original (antes de imputaciones y filtros)\n",
        "print('Antes de la limpieza:')\n",
        "df_original.describe(['quantity', 'unit_price']).show()\n",
        "\n",
        "# Estadísticas descriptivas del dataset limpio (tras imputación de nulos y eliminación de outliers)\n",
        "print('Después de la limpieza:')\n",
        "df.describe(['quantity', 'unit_price']).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o5evV-XAgb1"
      },
      "source": [
        "###  Conteo de Valores Únicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zwjBXMFBAgb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58b361a-af5e-4e03-c2a8-658464ad675d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ubicaciones únicas tras limpieza:\n",
            "+----------------+\n",
            "|  store_location|\n",
            "+----------------+\n",
            "|  sevilla-triana|\n",
            "|    bilbao-casco|\n",
            "|             nan|\n",
            "| valencia-puerto|\n",
            "|   madrid-centro|\n",
            "|barcelona -sants|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Verificación de valores únicos en 'store_location' tras limpieza ---\n",
        "\n",
        "print('Ubicaciones únicas tras limpieza:')\n",
        "df.select('store_location').distinct().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r3NMMVVxAgb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d85b0831-5a18-4df0-8213-ee924808305b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "|order_id|customer_id|product_id|quantity|unit_price|order_date|store_location|\n",
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "|       0|          0|         0|       0|         0|         0|             0|\n",
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "# --- Verificación final de valores nulos en el dataset limpio ---\n",
        "# Se asegura que no persistan valores nulos tras el proceso de limpieza\n",
        "df.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
        "]).show()\n",
        "\n",
        "# --- Persistencia del DataFrame limpio en formato Parquet ---\n",
        "# El formato Parquet es eficiente, comprimido y óptimo para análisis posteriores\n",
        "# Se sobrescribe el archivo si ya existe\n",
        "df.write.mode('overwrite').parquet('retail_sales_cleaned.parquet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Comprobaciones Finales y exportación de los datos"
      ],
      "metadata": {
        "id": "n_2TvRTQAUyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "# --- Comprobación final de valores nulos ---\n",
        "# Se confirma que tras el proceso de limpieza no quedan valores faltantes\n",
        "df.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
        "]).show()\n",
        "\n",
        "# --- Validación del tamaño final del dataset limpio ---\n",
        "# Se imprime el número total de registros tras eliminar duplicados y outliers\n",
        "print(f\"Número final de registros: {df.count()}\")\n",
        "\n",
        "# --- Exportación del dataset limpio en formato Parquet ---\n",
        "# Formato columnar optimizado para almacenamiento y análisis distribuido\n",
        "df.write.mode('overwrite').parquet('retail_sales_cleaned.parquet')"
      ],
      "metadata": {
        "id": "J085TU2-ANeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ae57db-8204-43f8-a076-739786a37d1e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "|order_id|customer_id|product_id|quantity|unit_price|order_date|store_location|\n",
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "|       0|          0|         0|       0|         0|         0|             0|\n",
            "+--------+-----------+----------+--------+----------+----------+--------------+\n",
            "\n",
            "Número final de registros: 494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J01GYDKdSIJq"
      },
      "source": [
        "##  7. Comparativa: Ventajas de Convertir CSV a Parquet\n",
        "\n",
        "En esta sección analizaremos las diferencias prácticas entre trabajar con un archivo CSV y un archivo Parquet en términos de **tamaño**, **velocidad de carga** y **eficiencia en la lectura de datos**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0akOhKC9SIJr"
      },
      "source": [
        "###  Comparar Tamaño de Archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sxQhAp-MSIJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475862a7-7955-4379-816b-ea1e6796bd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 32K Apr 23 17:23 retail_sales_complex.csv\n",
            "24K\tretail_sales_cleaned.parquet\n"
          ]
        }
      ],
      "source": [
        "# --- Comparación de tamaño en disco entre el dataset original (CSV) y el limpio (Parquet) ---\n",
        "\n",
        "# Tamaño del archivo CSV original\n",
        "!ls -lh retail_sales_complex.csv\n",
        "\n",
        "# Tamaño del dataset limpio exportado en formato Parquet\n",
        "!du -h retail_sales_cleaned.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcYUtt8MSIJr"
      },
      "source": [
        "###  Comparar Tiempo de Carga en PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aCwV8IoMSIJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6263c806-4baf-4f16-d556-6550ecd484b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de carga CSV: 0.5731 segundos\n",
            "Tiempo de carga Parquet: 0.5137 segundos\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# --- Comparación de tiempos de carga entre CSV y Parquet ---\n",
        "\n",
        "# Medición de tiempo para lectura del archivo CSV original\n",
        "start_csv = time.time()\n",
        "df_csv = spark.read.csv(\"retail_sales_complex.csv\", header=True, inferSchema=True)\n",
        "end_csv = time.time()\n",
        "print(f\"Tiempo de carga CSV: {end_csv - start_csv:.4f} segundos\")\n",
        "\n",
        "# Medición de tiempo para lectura del archivo Parquet (post-limpieza)\n",
        "start_parquet = time.time()\n",
        "df_parquet = spark.read.parquet(\"retail_sales_cleaned.parquet\")\n",
        "end_parquet = time.time()\n",
        "print(f\"Tiempo de carga Parquet: {end_parquet - start_parquet:.4f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4RkrmDCSIJs"
      },
      "source": [
        "###  Comparar Lectura de Columnas Específicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aanbAo82SIJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd4ba99-4e73-4b58-9790-08475e960a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+\n",
            "|order_id|quantity|\n",
            "+--------+--------+\n",
            "|    1195|     3.0|\n",
            "|    1079|     3.0|\n",
            "|    1480|     2.0|\n",
            "|    1109|     1.0|\n",
            "|    1280|     1.0|\n",
            "+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CSV → lectura columnas: 0.2171480655670166\n",
            "+--------+--------+\n",
            "|order_id|quantity|\n",
            "+--------+--------+\n",
            "|    1000|     2.0|\n",
            "|    1001|     2.0|\n",
            "|    1002|     1.0|\n",
            "|    1003|     2.0|\n",
            "|    1004|     3.0|\n",
            "+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Parquet → lectura columnas: 0.7865235805511475\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# --- Comparación de tiempos de lectura selectiva de columnas entre CSV y Parquet ---\n",
        "\n",
        "# Lectura de columnas específicas desde archivo CSV (formato fila a fila)\n",
        "start = time.time()\n",
        "df_csv.select(\"order_id\", \"quantity\").show(5)\n",
        "print(\"CSV → lectura columnas:\", time.time() - start)\n",
        "\n",
        "# Lectura de columnas específicas desde archivo Parquet (formato columnar optimizado)\n",
        "start = time.time()\n",
        "df_parquet.select(\"order_id\", \"quantity\").show(5)\n",
        "print(\"Parquet → lectura columnas:\", time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBGgjHdQSIJu"
      },
      "source": [
        "###  Conclusión\n",
        "\n",
        "Reflexiona sobre las diferencias observadas en términos de espacio, tiempos de carga y eficiencia.\n",
        "\n",
        "- ¿Qué ventajas ofrece el formato Parquet frente al CSV?\n",
        "- ¿En qué tipo de proyectos Big Data sería más adecuado usar Parquet?\n",
        "\n",
        "Incluye esta reflexión en tu informe final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw0_GAkwAgb2"
      },
      "source": [
        "##  8. Informe del Alumno\n",
        "- **Errores detectados:**\n",
        "- **Acciones aplicadas:**\n",
        "- **Impacto en el negocio:**\n",
        "- **Capturas de evidencia:**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}